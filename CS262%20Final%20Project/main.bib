@inproceedings{distbelief,
title = {Large Scale Distributed Deep Networks},
author  = {Jeffrey Dean and Greg S. Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Quoc V. Le and Mark Z. Mao and Marcâ€™Aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Andrew Y. Ng},
year  = 2012,
booktitle = {NIPS}
}
@article{bengio-emb,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 title = {A Neural Probabilistic Language Model},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {1137--1155},
 numpages = {19},
 url = {http://dl.acm.org/citation.cfm?id=944919.944966},
 acmid = {944966},
 publisher = {JMLR.org},
} 

@article{tensorflow,
  author    = {Mart{\'{\i}}n Abadi and
               Ashish Agarwal and
               Paul Barham and
               Eugene Brevdo and
               Zhifeng Chen and
               Craig Citro and
               Gregory S. Corrado and
               Andy Davis and
               Jeffrey Dean and
               Matthieu Devin and
               Sanjay Ghemawat and
               Ian J. Goodfellow and
               Andrew Harp and
               Geoffrey Irving and
               Michael Isard and
               Yangqing Jia and
               Rafal J{\'{o}}zefowicz and
               Lukasz Kaiser and
               Manjunath Kudlur and
               Josh Levenberg and
               Dan Mane and
               Rajat Monga and
               Sherry Moore and
               Derek Gordon Murray and
               Chris Olah and
               Mike Schuster and
               Jonathon Shlens and
               Benoit Steiner and
               Ilya Sutskever and
               Kunal Talwar and
               Paul A. Tucker and
               Vincent Vanhoucke and
               Vijay Vasudevan and
               Fernanda B. Vi{\'{e}}gas and
               Oriol Vinyals and
               Pete Warden and
               Martin Wattenberg and
               Martin Wicke and
               Yuan Yu and
               Xiaoqiang Zheng},
  title     = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
               Systems},
  journal   = {CoRR},
  volume    = {abs/1603.04467},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.04467},
  timestamp = {Sun, 03 Apr 2016 11:52:22 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/AbadiABBCCCDDDG16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@MISC{protobuf,
  title={Protocol Buffers},
  author={Kenton Varda},
  howpublished={\url{http://code.google.com/apis/protocolbuffers/}},
}
@techreport{adagrad,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010},
    Month = {Mar},
    URL = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html},
    Number = {UCB/EECS-2010-24},
    Abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods significantly outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
}